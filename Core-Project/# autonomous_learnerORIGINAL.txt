# autonomous_learner.py

import time
import random
from pathlib import Path
import json
from collections import defaultdict, Counter
from urllib.parse import urljoin, urlparse

from processing_nodes import LogicNode, SymbolicNode, DynamicBridge, CurriculumManager
import web_parser # Assuming web_parser.py contains fetch_raw_html, extract_links_with_text_from_html, clean_html_to_text, fetch_shallow
import parser as P_Parser # For chunk_content and extract_keywords

# --- Configuration ---
# --- Configuration ---
PHASE_URL_SOURCES = {
    1: [
        "https://en.wikipedia.org/wiki/Computer-aided_process_planning",
        "https://en.wikipedia.org/wiki/Computer-aided_production_engineering",
        "https://en.wikipedia.org/wiki/Digital_manufacturing",
        "https://en.wikipedia.org/wiki/Manufacturing_process_management",
        "https://en.wikipedia.org/wiki/Integrated_circuit_design",
        "https://en.wikipedia.org/wiki/Electronic_circuit",
        "https://en.wikipedia.org/wiki/Circuit_diagram",
        "https://en.wikipedia.org/wiki/Industrial_mineral",
        "https://en.wikipedia.org/wiki/Technology-critical_element",
        "https://en.wikipedia.org/wiki/Mineral"
    ],
    5: [
        "https://en.wikipedia.org/wiki/Transistor",
        "https://en.wikipedia.org/wiki/Photolithography",
        "https://en.wikipedia.org/wiki/Dielectric",
        "https://en.wikipedia.org/wiki/Solder",
        "https://en.wikipedia.org/wiki/Printed_electronics",
        "https://en.wikipedia.org/wiki/Surface-mount_technology",
        "https://en.wikipedia.org/wiki/DRAM",
        "https://en.wikipedia.org/wiki/Flash_memory",
        "https://en.wikipedia.org/wiki/Gallium_arsenide",
        "https://en.wikipedia.org/wiki/Tantalum"
    ]

    # For now, we will primarily focus on Phase 1. Other phases can be re-enabled later.
    # 2: ["https://en.wikipedia.org/wiki/Emotion", "https://en.wikipedia.org/wiki/Mythology"],
    # 3: ["https://en.wikipedia.org/wiki/History_of_science", "https://en.wikipedia.org/wiki/World_history"],
    # 4: ["https://en.wikipedia.org/wiki/Philosophy_of_mind", "https://en.wikipedia.org/wiki/Quantum_mechanics"]
}

deferred_urls_by_phase = defaultdict(list) # Will store lists of dictionaries
visited_urls_globally = set()
DATA_DIR = Path("data")

# --- Helper Functions ---
def initialize_data_files_if_needed():
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    data_files_to_check = {
        "data/vector_memory.json": [], "data/symbol_memory.json": {},
        "data/symbol_occurrence_log.json": {"entries": []},
        "data/symbol_emotion_map.json": {}, "data/meta_symbols.json": {},
        "data/trail_log.json": [],
        "data/deferred_urls_log.json": {},
        "data/seed_symbols.json": { # Basic seeds, can be expanded
            "üî•": {"name": "Fire", "keywords": ["fire", "flame", "computation", "logic"], "core_meanings": ["heat"], "emotions": ["anger"], "archetypes": ["destroyer"], "learning_phase": 0, "resonance_weight": 0.7},
            "üíß": {"name": "Water", "keywords": ["water", "liquid", "data", "flow"], "core_meanings": ["flow"], "emotions": ["calm"], "archetypes": ["healer"], "learning_phase": 0, "resonance_weight": 0.7},
            "üíª": {"name": "Computer", "keywords": ["computer", "computation", "cpu", "binary", "code", "algorithm", "system", "architecture"], "core_meanings": ["processing", "logic unit"], "emotions": ["neutral", "focus"], "archetypes": ["tool", "oracle"], "learning_phase": 0, "resonance_weight": 0.8}
        },
        "data/symbol_cooccurrence.json": {} # Initialize co-occurrence log
    }
    for file_path_str, default_content in data_files_to_check.items():
        file_path = Path(file_path_str)
        if not file_path.exists() or file_path.stat().st_size == 0:
            try:
                with open(file_path, "w", encoding="utf-8") as f: json.dump(default_content, f, indent=2, ensure_ascii=False)
                print(f"Initialized {file_path_str}")
            except Exception as e:
                print(f"Error initializing {file_path_str}: {e}")

        else: # File exists, check for corruption (simple check for JSON)
            if file_path_str not in ["data/seed_symbols.json"]:
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        if file_path_str == "data/deferred_urls_log.json":
                            loaded_data = json.load(f)
                            if not isinstance(loaded_data, dict):
                                raise json.JSONDecodeError("Not a dict", "", 0)
                        else:
                             json.load(f)
                except json.JSONDecodeError:
                    print(f"Warning: {file_path_str} was corrupted. Re-initializing.")
                    with open(file_path, "w", encoding="utf-8") as f: json.dump(default_content, f, indent=2, ensure_ascii=False)

def load_deferred_urls():
    global deferred_urls_by_phase
    deferred_log_path = DATA_DIR / "deferred_urls_log.json"
    if deferred_log_path.exists():
        try:
            with open(deferred_log_path, "r", encoding="utf-8") as f:
                loaded_deferred = json.load(f)
                for phase_key, url_list in loaded_deferred.items():
                    try:
                        phase_num = int(phase_key)
                        processed_url_list = []
                        for item in url_list:
                            if isinstance(item, str):
                                processed_url_list.append({
                                    "url": item, "anchor": Path(item).name,
                                    "priority_score_at_deferral": 1.0, "original_discovery_phase": 0
                                })
                            elif isinstance(item, dict) and "url" in item:
                                processed_url_list.append(item)
                        deferred_urls_by_phase[phase_num] = processed_url_list
                    except ValueError:
                        print(f"[WARN] Invalid phase key '{phase_key}' in {deferred_log_path}. Skipping.")
            # Moved this print statement to be the last line of the try block
            print(f"Loaded deferred URLs from {deferred_log_path}")
        except json.JSONDecodeError:
            print(f"[ERROR] Could not decode {deferred_log_path}. Starting with empty deferred URLs.")
            deferred_urls_by_phase = defaultdict(list)
        # Optional: Handle other potential exceptions like FileNotFoundError if not checking exists() first
        # except Exception as e: 
        #     print(f"[ERROR] An unexpected error occurred while loading deferred URLs: {e}")
        #     deferred_urls_by_phase = defaultdict(list)
    else:
        print(f"Deferred URLs log not found at {deferred_log_path}. Starting with empty deferred URLs.")
        deferred_urls_by_phase = defaultdict(list)

def save_deferred_urls():
    processed_deferred_urls = {}
    for phase, entries in deferred_urls_by_phase.items():
        unique_phase_entries = []
        seen_urls_in_phase = set()
        for entry_dict in entries:
            if isinstance(entry_dict, dict) and "url" in entry_dict:
                url_val = entry_dict["url"]
                if url_val not in seen_urls_in_phase:
                    unique_phase_entries.append(entry_dict)
                    seen_urls_in_phase.add(url_val)
        processed_deferred_urls[str(phase)] = unique_phase_entries
    try:
        with open(DATA_DIR / "deferred_urls_log.json", "w", encoding="utf-8") as f:
            json.dump(processed_deferred_urls, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Error saving deferred URLs: {e}")

def score_text_against_keywords(text_content, primary_keywords, secondary_keywords=None, anti_keywords=None):
    if not text_content or not isinstance(text_content, str): return 0.0, 0
    text_lower = text_content.lower()
    score = 0.0
    primary_matches = 0
    for kw in primary_keywords:
        if kw.lower() in text_lower: score += 2.0; primary_matches += 1
    if secondary_keywords:
        for kw in secondary_keywords:
            if kw.lower() in text_lower: score += 1.0
    if anti_keywords:
        for kw in anti_keywords:
            if kw.lower() in text_lower: score -= 3.0 # Strong penalty for anti-keywords
    return score, primary_matches

def evaluate_link_action(link_anchor_text, link_url,
                         current_processing_phase_num, curriculum_manager,
                         session_hot_keywords=None, force_phase1_focus=False):
    """
    Evaluates a link. If force_phase1_focus is True, it will heavily prioritize
    Phase 1 relevance and potentially ignore or deprioritize links for other phases.
    """
    current_phase_directives = curriculum_manager.get_processing_directives(current_processing_phase_num)
    phase1_directives = curriculum_manager.get_processing_directives(1) # Get Phase 1 specific directives

    # Score against current phase (static)
    static_score_current_phase, static_primary_matches_current_phase = score_text_against_keywords(
        link_anchor_text,
        current_phase_directives.get("phase_keywords_primary", []),
        current_phase_directives.get("phase_keywords_secondary", []),
        current_phase_directives.get("phase_keywords_anti", [])
    )

    # Score specifically against Phase 1 (static)
    static_score_phase1, static_primary_matches_phase1 = score_text_against_keywords(
        link_anchor_text,
        phase1_directives.get("phase_keywords_primary", []),
        phase1_directives.get("phase_keywords_secondary", []),
        phase1_directives.get("phase_keywords_anti", [])
    )

    dynamic_score = 0.0
    if session_hot_keywords:
        for hot_kw, freq in session_hot_keywords.items():
            if hot_kw.lower() in link_anchor_text.lower():
                dynamic_score += (1.0 * freq)
        dynamic_score = min(dynamic_score, current_phase_directives.get("max_dynamic_link_score_bonus", 5.0))

    # Use current phase weights for combining static and dynamic scores
    weight_static = current_phase_directives.get("link_score_weight_static", 0.6)
    weight_dynamic = current_phase_directives.get("link_score_weight_dynamic", 0.4)
    
    # Calculate combined score based on current phase context first
    final_priority_score_current_context = (static_score_current_phase * weight_static) + (dynamic_score * weight_dynamic)

    # Thresholds for current phase
    min_primary_follow_current = current_phase_directives.get("phase_min_primary_keyword_matches_for_link_follow", 1)
    min_total_score_follow_current = current_phase_directives.get("phase_min_total_keyword_score_for_link_follow", 2.5)

    # **MODIFICATION for Phase 1 Focus**
    if force_phase1_focus:
        # If forcing Phase 1 focus, only consider following if it's highly relevant to Phase 1
        # Use Phase 1's stricter thresholds for link following
        min_primary_follow_p1 = phase1_directives.get("phase_min_primary_keyword_matches_for_link_follow", 2)
        min_total_score_follow_p1 = phase1_directives.get("phase_min_total_keyword_score_for_link_follow", 5.0)

        if static_primary_matches_phase1 >= min_primary_follow_p1 and \
           ((static_score_phase1 * weight_static) + (dynamic_score * weight_dynamic)) >= min_total_score_follow_p1:
            # If it strongly matches Phase 1, follow it now (target phase is current, which should be 1)
            return "FOLLOW_NOW", current_processing_phase_num, ((static_score_phase1 * weight_static) + (dynamic_score * weight_dynamic))
        else:
            # If forcing Phase 1, and it doesn't strongly match Phase 1, ignore or defer to Phase 1 only if it has some Phase 1 relevance
            # This effectively stops exploration of non-Phase 1 topics for now
            min_primary_defer_p1 = phase1_directives.get("phase_min_primary_keyword_matches_for_link_follow", 1) # Lowered for deferral
            min_total_score_defer_p1 = phase1_directives.get("phase_min_total_keyword_score_for_link_follow", 2.0) # Lowered for deferral

            if static_primary_matches_phase1 >= min_primary_defer_p1 and \
               static_score_phase1 >= min_total_score_defer_p1 :
                 action = "DEFER_SHALLOW" if phase1_directives.get("allow_shallow_dive_for_future_phase_links", True) else "DEFER_URL_ONLY"
                 return action, 1, static_score_phase1 # Defer specifically to Phase 1
            return "IGNORE", None, static_score_phase1 # Ignore if not relevant to Phase 1

    # Original logic if not forcing Phase 1 focus
    if static_primary_matches_current_phase >= min_primary_follow_current and \
       final_priority_score_current_context >= min_total_score_follow_current:
        return "FOLLOW_NOW", current_processing_phase_num, final_priority_score_current_context

    best_future_phase_score = -float('inf')
    best_future_phase_num = None
    for future_phase_idx in range(1, curriculum_manager.get_max_phases() + 1):
        if future_phase_idx == current_processing_phase_num: continue
        future_phase_directives = curriculum_manager.get_processing_directives(future_phase_idx)
        defer_score, defer_primary_matches = score_text_against_keywords(
            link_anchor_text,
            future_phase_directives.get("phase_keywords_primary", []),
            future_phase_directives.get("phase_keywords_secondary", []),
            future_phase_directives.get("phase_keywords_anti", [])
        )
        min_primary_defer = future_phase_directives.get("phase_min_primary_keyword_matches_for_link_follow", 2)
        min_total_score_defer = future_phase_directives.get("phase_min_total_keyword_score_for_link_follow", 2.0)

        if defer_primary_matches >= min_primary_defer and defer_score >= min_total_score_defer:
            if defer_score > best_future_phase_score:
                best_future_phase_score = defer_score
                best_future_phase_num = future_phase_idx
    
    if best_future_phase_num is not None:
        action = "DEFER_SHALLOW" if current_phase_directives.get("allow_shallow_dive_for_future_phase_links", True) else "DEFER_URL_ONLY"
        return action, best_future_phase_num, best_future_phase_score
    
    return "IGNORE", None, final_priority_score_current_context


def autonomous_learning_cycle(focus_only_on_phase_1=True): # Added flag
    global deferred_urls_by_phase
    global visited_urls_globally

    initialize_data_files_if_needed()
    load_deferred_urls()

    logic_node = LogicNode()
    symbolic_node = SymbolicNode()
    curriculum_manager = CurriculumManager()
    dynamic_bridge = DynamicBridge(logic_node, symbolic_node, curriculum_manager)
    
    print(f"üöÄ Starting Autonomous Learning Cycle... (Phase 1 Focus: {focus_only_on_phase_1})")

    # Determine the range of phases to process
    phases_to_process = range(1, 2) if focus_only_on_phase_1 else range(1, curriculum_manager.get_max_phases() + 1)

    for current_phase_processing_num in phases_to_process:
        curriculum_manager.current_phase = current_phase_processing_num
        current_phase_directives = curriculum_manager.get_processing_directives(current_phase_processing_num)
        print(f"\nüåÄ =====  PHASE {current_phase_processing_num} ({current_phase_directives['info']})  =====")
        
        priority_queue_this_phase = []
        
        for seed_url in PHASE_URL_SOURCES.get(current_phase_processing_num, []):
            seed_anchor = Path(seed_url).name
            static_score, _ = score_text_against_keywords(seed_anchor,
                                                          current_phase_directives.get("phase_keywords_primary", []),
                                                          current_phase_directives.get("phase_keywords_secondary", []),
                                                          current_phase_directives.get("phase_keywords_anti", []))
            priority_queue_this_phase.append((static_score if static_score > 0 else 5.0, seed_url, seed_anchor, 0))

        phase_deferred_entries = []
        if current_phase_processing_num in deferred_urls_by_phase:
            phase_deferred_entries = deferred_urls_by_phase.pop(current_phase_processing_num)
        
        for deferred_entry in phase_deferred_entries:
            deferred_url = deferred_entry["url"]
            deferred_anchor = deferred_entry.get("anchor", Path(deferred_url).name)
            static_score, _ = score_text_against_keywords(
                deferred_anchor,
                current_phase_directives.get("phase_keywords_primary", []),
                current_phase_directives.get("phase_keywords_secondary", []),
                current_phase_directives.get("phase_keywords_anti", [])
            )
            priority_queue_this_phase.append((
                static_score if static_score > 0 else 3.0,
                deferred_url, deferred_anchor, 0
            ))

        temp_url_seen_in_queue = set()
        unique_priority_queue_items = []
        for item_tuple in priority_queue_this_phase:
            url_to_check = item_tuple[1]
            if url_to_check not in temp_url_seen_in_queue:
                unique_priority_queue_items.append(item_tuple)
                temp_url_seen_in_queue.add(url_to_check)
        priority_queue_this_phase = unique_priority_queue_items
        priority_queue_this_phase.sort(key=lambda x: x[0], reverse=True)

        urls_processed_in_session_count = 0
        # Use max_urls_to_process_per_phase_session from PHASE 1 directives if focus_only_on_phase_1 is true
        if focus_only_on_phase_1 and current_phase_processing_num == 1:
            phase1_specific_directives = curriculum_manager.get_processing_directives(1)
            max_urls_for_session = phase1_specific_directives.get("max_urls_to_process_per_phase_session", 10)
        else:
            max_urls_for_session = current_phase_directives.get("max_urls_to_process_per_phase_session", 2)

        
        session_hot_keywords = Counter()
        MAX_HOT_KEYWORDS = current_phase_directives.get("max_session_hot_keywords", 20)
        MIN_HOT_KEYWORD_FREQ = current_phase_directives.get("min_session_hot_keyword_freq", 2)

        while priority_queue_this_phase and urls_processed_in_session_count < max_urls_for_session:
            current_priority, current_url, current_anchor, current_hop_count = priority_queue_this_phase.pop(0)

            if current_url in visited_urls_globally: continue
            max_hop_depth = current_phase_directives.get("max_exploration_depth_from_seed_url", 5)
            if current_hop_count > max_hop_depth:
                print(f"    Max hop depth ({current_hop_count}/{max_hop_depth}) reached for {current_url}. Skipping.")
                continue
            
            print(f"\nüîó Processing URL ({urls_processed_in_session_count + 1}/{max_urls_for_session}, hop {current_hop_count}, prio {current_priority:.2f}): {current_url}")
            visited_urls_globally.add(current_url)
            urls_processed_in_session_count += 1
            curriculum_manager.update_metrics(current_phase_processing_num, urls_visited_increment=1)

            raw_html_content = web_parser.fetch_raw_html(current_url)
            if not raw_html_content: time.sleep(random.uniform(1,2)); continue
            
            extracted_links_with_anchor = web_parser.extract_links_with_text_from_html(current_url, raw_html_content)
            main_page_text = web_parser.clean_html_to_text(raw_html_content)
            if not main_page_text: time.sleep(random.uniform(1,2)); continue

            page_chunks = P_Parser.chunk_content(main_page_text, max_chunk_size=1000)
            print(f"    üìÑ Page yielded {len(page_chunks)} chunks. Processing...")
            
            for chunk_num, chunk in enumerate(page_chunks):
                chunk_score_current_phase, prim_matches_current = score_text_against_keywords(
                    chunk,
                    current_phase_directives.get("phase_keywords_primary", []),
                    current_phase_directives.get("phase_keywords_secondary", []),
                    current_phase_directives.get("phase_keywords_anti", [])
                )
                is_relevant_for_current_processing = (prim_matches_current >= current_phase_directives.get("phase_min_primary_keyword_matches_for_chunk_relevance", 1) and \
                                                      chunk_score_current_phase >= current_phase_directives.get("phase_min_total_keyword_score_for_chunk_relevance", 1.0))

                # If focusing only on phase 1, ensure target_storage_phase is 1 or the chunk is ignored/handled by strict deferral.
                if focus_only_on_phase_1 and current_phase_processing_num == 1:
                    target_storage_phase_for_chunk = dynamic_bridge.determine_target_storage_phase(chunk, 1)
                    # If strict phase 1 logic defers it to phase 2, but we are only doing phase 1, it effectively might not be stored for phase 1.
                    # The determine_target_storage_phase and route_chunk_for_processing should handle this.
                    # If it's not relevant for Phase 1 keywords at all, it might get a low score and not be processed deeply.
                    if not is_relevant_for_current_processing and target_storage_phase_for_chunk != 1:
                        # print(f"    Chunk {chunk_num+1} not relevant for Phase 1 focus and targets phase {target_storage_phase_for_chunk}. Skipping deep processing for now.")
                        # continue # Or just let route_chunk_for_processing handle it based on target_storage_phase
                        pass

                else:
                    target_storage_phase_for_chunk = dynamic_bridge.determine_target_storage_phase(chunk, current_phase_processing_num)
                
                dynamic_bridge.route_chunk_for_processing(
                    text_input=chunk, source_url=current_url,
                    current_processing_phase=current_phase_processing_num,
                    target_storage_phase=target_storage_phase_for_chunk,
                    is_highly_relevant_for_current_phase=is_relevant_for_current_processing,
                    is_shallow_content=False
                )
                if is_relevant_for_current_processing: # Only update hot keywords if relevant to current processing phase
                    chunk_kws = P_Parser.extract_keywords(chunk, max_keywords=5)
                    session_hot_keywords.update(chunk_kws)
            
            if session_hot_keywords:
                 pruned_hot_keywords = Counter({
                     kw: count for kw, count in session_hot_keywords.most_common(MAX_HOT_KEYWORDS * 2)
                     if count >= MIN_HOT_KEYWORD_FREQ
                 })
                 session_hot_keywords = Counter(dict(pruned_hot_keywords.most_common(MAX_HOT_KEYWORDS)))
                 if session_hot_keywords: print(f"    üî• Session Hot Keywords (Top {len(session_hot_keywords)}): {list(session_hot_keywords.keys())}")

            new_links_to_consider_tuples = []
            for link_url_abs, link_anchor_text in extracted_links_with_anchor:
                try:
                    parsed_link_domain = urlparse(link_url_abs).netloc
                    parsed_current_domain = urlparse(current_url).netloc
                    # Allow same domain or common subdomains like en.wikipedia.org from www.wikipedia.org
                    if not (parsed_link_domain == parsed_current_domain or parsed_link_domain.endswith("." + parsed_current_domain) or parsed_current_domain.endswith("." + parsed_link_domain) ):
                        # print(f"    Skipping cross-domain link: {link_url_abs}")
                        continue
                except Exception: continue
                if link_url_abs in visited_urls_globally: continue

                action, target_phase_for_link, link_priority_score = evaluate_link_action(
                    link_anchor_text, link_url_abs, current_phase_processing_num, curriculum_manager, session_hot_keywords,
                    force_phase1_focus=(focus_only_on_phase_1 and current_phase_processing_num == 1) # Pass the focus flag
                )

                if action == "FOLLOW_NOW":
                    # If focusing on Phase 1, only add if target_phase_for_link is 1
                    if focus_only_on_phase_1 and current_phase_processing_num == 1 and target_phase_for_link != 1:
                        # print(f"    Phase 1 Focus: Ignoring FOLLOW_NOW for link to target phase {target_phase_for_link}: {link_url_abs}")
                        # Check if it's worth deferring to Phase 1 instead, even if `evaluate_link_action` didn't say so with force_phase1_focus
                        # This could happen if evaluate_link_action with force_phase1_focus decided to IGNORE but it still has some P1 relevance.
                        # However, evaluate_link_action with force_phase1_focus should ideally handle this.
                        pass
                    else:
                        new_links_to_consider_tuples.append((link_priority_score, link_url_abs, link_anchor_text, current_hop_count + 1))

                elif action == "DEFER_SHALLOW" or action == "DEFER_URL_ONLY":
                    # If focusing on Phase 1, only defer if target_phase_for_link is 1
                    if focus_only_on_phase_1 and current_phase_processing_num == 1 and target_phase_for_link != 1:
                        # print(f"    Phase 1 Focus: Ignoring DEFER for link to target phase {target_phase_for_link}: {link_url_abs}")
                        pass # Don't defer to other phases if strictly focusing on Phase 1
                    else: # Original deferral logic
                        is_already_deferred = False
                        for existing_deferred_list in deferred_urls_by_phase.values():
                            if any(entry.get("url") == link_url_abs for entry in existing_deferred_list if isinstance(entry, dict)):
                                is_already_deferred = True; break
                        if not is_already_deferred:
                            deferred_urls_by_phase[target_phase_for_link].append({
                                "url": link_url_abs, "anchor": link_anchor_text,
                                "priority_score_at_deferral": link_priority_score,
                                "original_discovery_phase": current_phase_processing_num,
                                "original_discovery_hop_count": current_hop_count +1
                            })
                        if action == "DEFER_SHALLOW" and link_url_abs not in visited_urls_globally:
                            shallow_content = web_parser.fetch_shallow(link_url_abs, max_chars=current_phase_directives.get("shallow_dive_max_chars", 500))
                            if shallow_content:
                                dynamic_bridge.route_chunk_for_processing(
                                    text_input=shallow_content, source_url=link_url_abs,
                                    current_processing_phase=current_phase_processing_num,
                                    target_storage_phase=target_phase_for_link,
                                    is_highly_relevant_for_current_phase=False,
                                    is_shallow_content=True
                                )
            
            if new_links_to_consider_tuples:
                print(f"    üîó Adding {len(new_links_to_consider_tuples)} new links to consider for phase queue. Re-prioritizing...")
                newly_added_count = 0
                for p_score, n_url, n_anchor, n_hop in new_links_to_consider_tuples:
                    if not any(item[1] == n_url for item in priority_queue_this_phase):
                        priority_queue_this_phase.append((p_score, n_url, n_anchor, n_hop))
                        newly_added_count +=1
                if newly_added_count > 0:
                    priority_queue_this_phase.sort(key=lambda x: x[0], reverse=True)

            time.sleep(random.uniform(0.5, 1.0)) # Reduced polite delay slightly

        print(f"üèÅ Phase {current_phase_processing_num} scraping session complete. Processed {urls_processed_in_session_count} URLs.")
        if not focus_only_on_phase_1 or current_phase_processing_num == 1: # Only run meta-analysis if relevant phase processed
            print(f"üî¨ Running meta-symbol analysis for data up to phase {current_phase_processing_num}...")
            symbolic_node.run_meta_symbol_analysis(max_phase_to_consider=current_phase_processing_num)
        
        try: save_deferred_urls()
        except Exception as e_save: print(f"[ERROR] Could not save deferred URLs after phase {current_phase_processing_num}: {e_save}")

        if not focus_only_on_phase_1: # Only advance if not strictly focusing on phase 1
            if curriculum_manager.advance_phase_if_ready(current_phase_processing_num):
                 print(f"üéâ Advanced to Phase {curriculum_manager.get_current_phase()} based on metrics!")
            else:
                if current_phase_processing_num < curriculum_manager.get_max_phases():
                     print(f"üìä Metrics not yet met to advance from Phase {current_phase_processing_num}.")
                elif current_phase_processing_num == curriculum_manager.get_max_phases():
                     print("üèÅ All curriculum phases processed in this learning cycle.")
        elif current_phase_processing_num == 1 and focus_only_on_phase_1:
            print(f"üèÅ Phase 1 (Focus Mode) processing complete.")


    print("\n‚úÖ Autonomous Learning Cycle Finished.")
    save_deferred_urls()
    print(f"Total URLs visited globally during this cycle: {len(visited_urls_globally)}")

if __name__ == "__main__":
    # Default to Phase 1 focus, can be changed via command line arg or other config later if needed
    phase_1_only_focus = True 
    print(f"Starting autonomous learning process... Phase 1 Focus: {phase_1_only_focus}")
    try:
        autonomous_learning_cycle(focus_only_on_phase_1=phase_1_only_focus)
    except KeyboardInterrupt:
        print("\nüõë Autonomous learning interrupted by user.")
        save_deferred_urls()
    except Exception as e:
        print(f"üí• An error occurred during autonomous learning: {e}")
        import traceback
        traceback.print_exc()
        save_deferred_urls()